[architecture]
input_size=768
hidden_layers=3
hidden_sizes=1024,512,256
output_size=6

[hyperparameters]
learning_rate=0.01897115542102495
batch_size=256
activation=relu
dropout=0.31308666033026233
epochs=1000
samples_per_epoch=6656

[initialization]
weight_init=he
bias_init=zeros

[lr_scheduler]
type=exponential
initial_lr=0.01
decay_rate=0.9499667896735509
decay_steps=6
min_lr=0.0009868732583552429


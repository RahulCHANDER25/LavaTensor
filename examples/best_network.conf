[architecture]
input_size=768
hidden_layers=3
hidden_sizes=64,16,16
output_size=6

[hyperparameters]
learning_rate=0.029544920911964197
batch_size=224
activation=relu
dropout=0.422381251446263
epochs=50
samples_per_epoch=10304

[initialization]
weight_init=he
bias_init=zeros

[lr_scheduler]
type=exponential
initial_lr=0.01
decay_rate=0.837377623366803
decay_steps=5
min_lr=3.8549390490672925e-05


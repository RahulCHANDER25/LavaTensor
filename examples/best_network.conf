[architecture]
input_size=768
hidden_layers=3
hidden_sizes=8,8,8
output_size=6

[hyperparameters]
learning_rate=0.010214962136071731
batch_size=80
activation=relu
dropout=0.28481850661823976
epochs=1
samples_per_epoch=640

[initialization]
weight_init=he
bias_init=zeros

[lr_scheduler]
type=exponential
initial_lr=0.01
decay_rate=0.8874724743280968
decay_steps=4
min_lr=0.0001577481860135783


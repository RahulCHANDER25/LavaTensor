[architecture]
input_size=768
hidden_layers=2
hidden_sizes=512,256
output_size=6

[hyperparameters]
learning_rate=0.022215673448668344
batch_size=240
activation=relu
dropout=0.4256939752715408
epochs=1000
samples_per_epoch=6000

[initialization]
weight_init=he
bias_init=zeros

[lr_scheduler]
type=exponential
initial_lr=0.01
decay_rate=0.983667764937219
decay_steps=4
min_lr=0.0009126742519640651

